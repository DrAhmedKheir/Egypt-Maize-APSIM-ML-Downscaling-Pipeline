{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# APSIM-NG + ML Downscaling (Egypt Maize) — Jupyter Notebook\n",
        "\n",
        "**Instructions:** Run cells top-to-bottom. Edit paths in **Section 0**.\n",
        "\n",
        "Notes:\n",
        "- If APSIM is not runnable in your environment, set `DO_RUN_APSIM = False` and ensure `.db` outputs exist.\n",
        "- This notebook was converted from your R workflow into Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) USER SETTINGS (edit these paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "# If using Google Drive:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# BASE_DIR = \"/content/drive/MyDrive/APSIMICARDATraining/AnotherFullScript\"\n",
        "\n",
        "BASE_DIR = \"/content\"  # change\n",
        "WORK_DIR = os.path.join(BASE_DIR, \"EG_Maize_CMIP6_FULL\")\n",
        "COARSE_DIR = os.path.join(WORK_DIR, \"coarse\")\n",
        "OUT_DIR = os.path.join(WORK_DIR, \"outputs_master\")\n",
        "CACHE_DIR = os.path.join(WORK_DIR, \"_cache\")\n",
        "\n",
        "APSIM_TEMPLATE = \"MaizeFull.apsimx\"\n",
        "DB_NAME_PREF = \"MaizeFull.db\"\n",
        "\n",
        "# Fine grid resolution for downscaling\n",
        "RES_FINE = 0.1\n",
        "\n",
        "# Fail threshold (t/ha)\n",
        "YIELD_FAIL_THRESHOLD = 3.0\n",
        "\n",
        "# Cores\n",
        "N_CORES = max(1, (os.cpu_count() or 2) - 1)\n",
        "\n",
        "# Climate feature season months\n",
        "SEASON_MONTHS = {3, 4, 5, 6, 7, 8}\n",
        "\n",
        "# CMIP6 fallback deltas (replace later with real deltas if you have them)\n",
        "CMIP6 = pd.DataFrame({\"scenario\": [\"SSP245\", \"SSP585\"], \"dT\": [2.1, 3.8]})\n",
        "\n",
        "# Uncertainty / probability-of-failure bootstrap settings\n",
        "DO_BOOTSTRAP_UNCERTAINTY = True\n",
        "N_BOOT = 50  # 30–80 typical\n",
        "\n",
        "# SoilGrids settings\n",
        "DO_SOILGRIDS = True\n",
        "SOIL_DEPTHS_CM = [\"0-5\", \"5-15\", \"15-30\"]  # will compute weighted 0–30cm\n",
        "SOIL_PROPS = [\"sand\", \"clay\", \"silt\", \"soc\", \"bdod\", \"phh2o\", \"cec\"]\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def msg(*args):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] \" + (\" \".join(str(a) for a in args)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) HELPERS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "def to_float(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "        if not math.isfinite(v):\n",
        "            return np.nan\n",
        "        return v\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def impute_median(arr: pd.Series) -> pd.Series:\n",
        "    s = pd.to_numeric(arr, errors=\"coerce\")\n",
        "    med = np.nanmedian(s.to_numpy(dtype=float))\n",
        "    if not np.isfinite(med):\n",
        "        return s\n",
        "    return s.fillna(med)\n",
        "\n",
        "\n",
        "def xy_matrix(lon: np.ndarray, lat: np.ndarray) -> np.ndarray:\n",
        "    m = np.column_stack([lon.astype(float), lat.astype(float)])\n",
        "    return m\n",
        "\n",
        "\n",
        "def idw_knn(src_xy: np.ndarray, src_val: np.ndarray, trg_xy: np.ndarray, k: int = 8, power: float = 2.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    kNN inverse-distance weighted interpolation (IDW).\n",
        "    \"\"\"\n",
        "    src_val = np.asarray(src_val, dtype=float)\n",
        "    if np.all(np.isnan(src_val)):\n",
        "        raise ValueError(\"All NA values in interpolation source\")\n",
        "\n",
        "    k_eff = min(k, src_xy.shape[0])\n",
        "    nn = NearestNeighbors(n_neighbors=k_eff, algorithm=\"auto\")\n",
        "    nn.fit(src_xy)\n",
        "    d, idx = nn.kneighbors(trg_xy, return_distance=True)\n",
        "\n",
        "    d = np.where(d == 0, 1e-12, d)\n",
        "    w = 1.0 / (d ** power)\n",
        "\n",
        "    v = src_val[idx]\n",
        "    num = np.nansum(w * v, axis=1)\n",
        "    den = np.nansum(w, axis=1)\n",
        "    out = num / den\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) INDEX COARSE SITES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Indexing coarse site folders...\")\n",
        "\n",
        "site_dirs = [p for p in sorted(Path(COARSE_DIR).glob(\"EG_[0-9][0-9][0-9][0-9]\")) if p.is_dir()]\n",
        "run_index = pd.DataFrame({\n",
        "    \"site_id\": [p.name for p in site_dirs],\n",
        "    \"site_dir\": [str(p) for p in site_dirs],\n",
        "})\n",
        "run_index[\"apsimx_fp\"] = run_index[\"site_dir\"].apply(lambda d: str(Path(d) / APSIM_TEMPLATE))\n",
        "run_index = run_index[run_index[\"apsimx_fp\"].apply(lambda fp: Path(fp).exists())].reset_index(drop=True)\n",
        "\n",
        "if len(run_index) == 0:\n",
        "    raise FileNotFoundError(f\"No site folders found with {APSIM_TEMPLATE} under: {COARSE_DIR}\")\n",
        "\n",
        "msg(f\"Found {len(run_index)} site(s). Example:\", run_index.loc[0, \"apsimx_fp\"])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) EDIT REPORT NODE IN .apsimx (JSON)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "REPORT_VARS = [\n",
        "    \"[Clock].Today as Date\",\n",
        "    \"[Maize].Grain.Total.Wt*10 as Yield\",   # g/m2 -> kg/ha (x10)\n",
        "    \"[Maize].Leaf.LAI as LAI\",\n",
        "    \"[Maize].AboveGround.Wt as Biomass\",\n",
        "]\n",
        "\n",
        "def _walk_json(node, fn):\n",
        "    \"\"\"\n",
        "    DFS walk through dict/list json structure; apply fn to each dict node.\n",
        "    \"\"\"\n",
        "    if isinstance(node, dict):\n",
        "        fn(node)\n",
        "        for k, v in node.items():\n",
        "            _walk_json(v, fn)\n",
        "    elif isinstance(node, list):\n",
        "        for v in node:\n",
        "            _walk_json(v, fn)\n",
        "\n",
        "def ensure_report(apsimx_fp: str) -> None:\n",
        "    \"\"\"\n",
        "    Set Report.EventNames and Report.VariableNames.\n",
        "    This mirrors apsimx::edit_apsimx(node=\"Report\", parm=...) in the R script.\n",
        "\n",
        "    Assumption: APSIM .apsimx is JSON where a node has \"Name\": \"Report\"\n",
        "    and properties keys may vary by APSIM version. We handle common patterns.\n",
        "    \"\"\"\n",
        "    p = Path(apsimx_fp)\n",
        "    obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "    found = {\"n\": 0}\n",
        "\n",
        "    def edit_if_report(d: dict):\n",
        "        if d.get(\"Name\", \"\").lower() == \"report\":\n",
        "            found[\"n\"] += 1\n",
        "            # Common APSIM schema uses \"EventNames\" and \"VariableNames\"\n",
        "            d[\"EventNames\"] = [\"EndOfDay\"]\n",
        "            d[\"VariableNames\"] = REPORT_VARS\n",
        "\n",
        "    _walk_json(obj, edit_if_report)\n",
        "\n",
        "    if found[\"n\"] == 0:\n",
        "        raise ValueError(f\"Could not find a JSON node with Name=='Report' in {apsimx_fp}. \"\n",
        "                         \"Open the .apsimx (it is JSON) and confirm the Report node name.\")\n",
        "\n",
        "    p.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) RUN APSIM (OPTIONAL): call Models executable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "def guess_models_exe() -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Try to locate APSIM NextGen Models executable on Linux.\n",
        "    Common paths after .deb install include /usr/local/bin/Models.\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        \"/usr/local/bin/Models\",\n",
        "        \"/usr/local/bin/Models.exe\",\n",
        "        \"/usr/bin/Models\",\n",
        "        \"/usr/bin/Models.exe\",\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        if Path(c).exists():\n",
        "            return c\n",
        "    return shutil.which(\"Models\") or shutil.which(\"Models.exe\")\n",
        "\n",
        "def run_apsim_safely(apsimx_fp: str, models_exe: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Run an APSIM .apsimx file. If models_exe is None, we try to auto-detect.\n",
        "    \"\"\"\n",
        "    models_exe = models_exe or guess_models_exe()\n",
        "    if not models_exe:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not find APSIM Models executable. \"\n",
        "            \"Install APSIM NG on Linux (.deb) or set models_exe explicitly.\"\n",
        "        )\n",
        "    cmd = [models_exe, apsimx_fp]\n",
        "    # APSIM writes output to <apsimx>.db by default\n",
        "    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) ROBUST DB READ (SQLite)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "def read_report_from_db(site_dir: str, db_name: str = DB_NAME_PREF) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Read first Report-like table from APSIM SQLite .db.\n",
        "    \"\"\"\n",
        "    site_dir = str(site_dir)\n",
        "    db_fp = Path(site_dir) / db_name\n",
        "    if not db_fp.exists():\n",
        "        dbs = list(Path(site_dir).glob(\"*.db\"))\n",
        "        if not dbs:\n",
        "            return None\n",
        "        db_fp = dbs[0]\n",
        "\n",
        "    con = sqlite3.connect(str(db_fp))\n",
        "    try:\n",
        "        tabs = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", con)[\"name\"].tolist()\n",
        "        if not tabs:\n",
        "            return None\n",
        "        rep_tabs = [t for t in tabs if not t.startswith(\"_\")]\n",
        "        pick = [t for t in rep_tabs if re.match(r\"^Report\", t, flags=re.IGNORECASE)]\n",
        "        if not pick:\n",
        "            pick = rep_tabs\n",
        "        if not pick:\n",
        "            return None\n",
        "        df = pd.read_sql(f\"SELECT * FROM [{pick[0]}]\", con)\n",
        "        if df.empty:\n",
        "            return None\n",
        "        if \"Date\" in df.columns:\n",
        "            # APSIM often stores datetime strings\n",
        "            df[\"Date\"] = pd.to_datetime(df[\"Date\"].astype(str).str.replace(r\"\\s.*\", \"\", regex=True), errors=\"coerce\").dt.date\n",
        "        return df\n",
        "    finally:\n",
        "        con.close()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) COORDS (from .met header) + MET READER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "def parse_met_lonlat(site_dir: str) -> Tuple[float, float]:\n",
        "    mets = list(Path(site_dir).glob(\"*.met\"))\n",
        "    if not mets:\n",
        "        return (np.nan, np.nan)\n",
        "    x = mets[0].read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "\n",
        "    lat_line = next((l for l in x if re.search(r\"latitude\", l, re.IGNORECASE)), None)\n",
        "    lon_line = next((l for l in x if re.search(r\"longitude\", l, re.IGNORECASE)), None)\n",
        "\n",
        "    def grab_num(s: Optional[str]) -> float:\n",
        "        if not s:\n",
        "            return np.nan\n",
        "        m = re.search(r\"(-?\\d+\\.?\\d*)\", s)\n",
        "        return to_float(m.group(1)) if m else np.nan\n",
        "\n",
        "    return (grab_num(lon_line), grab_num(lat_line))\n",
        "\n",
        "\n",
        "def read_met_data(site_dir: str) -> Optional[pd.DataFrame]:\n",
        "    mets = list(Path(site_dir).glob(\"*.met\"))\n",
        "    if not mets:\n",
        "        return None\n",
        "    lines = mets[0].read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "\n",
        "    # find header line starting with \"year day\"\n",
        "    i0 = None\n",
        "    for i, l in enumerate(lines):\n",
        "        if re.match(r\"^\\s*year\\s+day\\b\", l, flags=re.IGNORECASE):\n",
        "            i0 = i\n",
        "            break\n",
        "    if i0 is None:\n",
        "        return None\n",
        "\n",
        "    hdr = re.sub(r\"\\s+\", \" \", lines[i0].strip().lower())\n",
        "    cols = hdr.split(\" \")\n",
        "\n",
        "    start_line = i0 + 2  # skip units line\n",
        "    if start_line >= len(lines):\n",
        "        return None\n",
        "\n",
        "    # fixed-width whitespace\n",
        "    data_txt = \"\\n\".join(lines[start_line:])\n",
        "    df = pd.read_csv(\n",
        "        pd.io.common.StringIO(data_txt),\n",
        "        delim_whitespace=True,\n",
        "        header=None,\n",
        "        engine=\"python\",\n",
        "    )\n",
        "    if df.shape[1] < len(cols):\n",
        "        return None\n",
        "    df = df.iloc[:, :len(cols)]\n",
        "    df.columns = cols\n",
        "\n",
        "    for nm in set([\"year\", \"day\", \"maxt\", \"mint\", \"radn\", \"rain\", \"rh\", \"windspeed\"]).intersection(df.columns):\n",
        "        df[nm] = pd.to_numeric(df[nm], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def calc_DD35(tmax: np.ndarray) -> float:\n",
        "    return float(np.nansum(np.maximum(tmax - 35.0, 0.0)))\n",
        "\n",
        "\n",
        "def calc_HDW(tmax: np.ndarray, tmin: np.ndarray) -> float:\n",
        "    tmean = (tmax + tmin) / 2.0\n",
        "    return float(np.nansum((tmax > 35.0) & (tmean > 30.0)))\n",
        "\n",
        "\n",
        "def compute_climate_features_site(site_id: str, site_dir: str, season_months: Optional[set] = None) -> Optional[pd.DataFrame]:\n",
        "    met = read_met_data(site_dir)\n",
        "    if met is None or not {\"year\", \"day\"}.issubset(met.columns):\n",
        "        return None\n",
        "\n",
        "    # build dates to filter months\n",
        "    # date = origin + (day-1)\n",
        "    d = pd.to_datetime(met[\"year\"].astype(int).astype(str) + \"-01-01\", errors=\"coerce\") + pd.to_timedelta(met[\"day\"] - 1, unit=\"D\")\n",
        "\n",
        "    if season_months is not None:\n",
        "        keep = d.dt.month.isin(sorted(list(season_months)))\n",
        "        met = met.loc[keep].copy()\n",
        "        d = d.loc[keep]\n",
        "\n",
        "    if met.empty:\n",
        "        return None\n",
        "\n",
        "    ny = met[\"year\"].nunique()\n",
        "    if not np.isfinite(ny) or ny <= 0:\n",
        "        ny = 1\n",
        "\n",
        "    tmax = met.get(\"maxt\", pd.Series(dtype=float)).to_numpy(dtype=float)\n",
        "    tmin = met.get(\"mint\", pd.Series(dtype=float)).to_numpy(dtype=float)\n",
        "    radn = met.get(\"radn\", pd.Series(dtype=float)).to_numpy(dtype=float)\n",
        "    rain = met.get(\"rain\", pd.Series(dtype=float)).to_numpy(dtype=float)\n",
        "    rh   = met.get(\"rh\",   pd.Series(dtype=float)).to_numpy(dtype=float)\n",
        "    wind = met.get(\"windspeed\", pd.Series(dtype=float)).to_numpy(dtype=float)\n",
        "\n",
        "    out = {\n",
        "        \"site_id\": site_id,\n",
        "        \"DD35\": calc_DD35(tmax) / ny,\n",
        "        \"HDW\":  calc_HDW(tmax, tmin) / ny,\n",
        "        \"tmax_mean\": float(np.nanmean(tmax)),\n",
        "        \"tmin_mean\": float(np.nanmean(tmin)),\n",
        "        \"tmean_mean\": float(np.nanmean((tmax + tmin) / 2.0)),\n",
        "        \"tmax_p95\": float(np.nanpercentile(tmax, 95)),\n",
        "        \"tmin_p05\": float(np.nanpercentile(tmin, 5)),\n",
        "        \"radn_mean\": float(np.nanmean(radn)),\n",
        "        \"rain_sum\": float(np.nansum(rain) / ny),\n",
        "        \"rain_p95\": float(np.nanpercentile(rain, 95)),\n",
        "        \"rh_mean\": float(np.nanmean(rh)),\n",
        "        \"wind_mean\": float(np.nanmean(wind)),\n",
        "        \"wind_p95\": float(np.nanpercentile(wind, 95)),\n",
        "    }\n",
        "    return pd.DataFrame([out])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) SOIL (ISRIC SoilGrids v2 API)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "import requests\n",
        "\n",
        "def soilgrids_point(lon: float, lat: float,\n",
        "                    props: List[str] = SOIL_PROPS,\n",
        "                    depths: List[str] = SOIL_DEPTHS_CM,\n",
        "                    timeout_sec: int = 60) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Always returns exactly 1 row (NA if request fails), to keep joins safe.\n",
        "    Weighted aggregation to 0–30 cm using (0-5,5-15,15-30) weights 5,10,15.\n",
        "    \"\"\"\n",
        "    base_url = \"https://rest.isric.org/soilgrids/v2.0/properties/query\"\n",
        "    out_cols = [f\"{p}_0_30\" for p in props]\n",
        "\n",
        "    weights = {\"0-5\": 5, \"5-15\": 10, \"15-30\": 15}\n",
        "    w = np.array([weights[d] for d in depths], dtype=float)\n",
        "    w = w / w.sum()\n",
        "\n",
        "    out_fail = pd.DataFrame([{c: np.nan for c in out_cols}])\n",
        "\n",
        "    if not (np.isfinite(lon) and np.isfinite(lat)):\n",
        "        return out_fail\n",
        "\n",
        "    params = {\n",
        "        \"lon\": lon,\n",
        "        \"lat\": lat,\n",
        "        \"property\": props,\n",
        "        \"depth\": depths,\n",
        "        \"value\": \"mean\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(base_url, params=params, timeout=timeout_sec)\n",
        "        if r.status_code != 200:\n",
        "            return out_fail\n",
        "        js = r.json()\n",
        "        if \"properties\" not in js:\n",
        "            return out_fail\n",
        "\n",
        "        def get_mean(prop: str, depth: str) -> float:\n",
        "            p = js[\"properties\"].get(prop)\n",
        "            if not p or \"depths\" not in p:\n",
        "                return np.nan\n",
        "            top, bot = depth.split(\"-\")\n",
        "            top = int(top); bot = int(bot)\n",
        "            for dd in p[\"depths\"]:\n",
        "                rg = dd.get(\"range\", {})\n",
        "                if rg.get(\"top_depth\") == top and rg.get(\"bottom_depth\") == bot:\n",
        "                    v = dd.get(\"values\", {}).get(\"mean\")\n",
        "                    return to_float(v)\n",
        "            return np.nan\n",
        "\n",
        "        out = {}\n",
        "        for prop in props:\n",
        "            vals = np.array([get_mean(prop, d) for d in depths], dtype=float)\n",
        "            out[f\"{prop}_0_30\"] = float(np.nansum(vals * w))\n",
        "\n",
        "        # ensure full colset\n",
        "        for c in out_cols:\n",
        "            out.setdefault(c, np.nan)\n",
        "\n",
        "        return pd.DataFrame([out])[out_cols]\n",
        "\n",
        "    except Exception:\n",
        "        return out_fail\n",
        "\n",
        "\n",
        "def get_soil_features_for_sites(site_tbl: pd.DataFrame, cache_dir: str) -> pd.DataFrame:\n",
        "    soil_cache_fp = Path(cache_dir) / \"soilgrids_coarse_sites.csv\"\n",
        "    out_cols = [\"site_id\"] + [f\"{p}_0_30\" for p in SOIL_PROPS]\n",
        "\n",
        "    if soil_cache_fp.exists():\n",
        "        sc = pd.read_csv(soil_cache_fp)\n",
        "        if \"site_id\" in sc.columns and set(site_tbl[\"site_id\"]).issubset(set(sc[\"site_id\"])):\n",
        "            for c in out_cols:\n",
        "                if c not in sc.columns:\n",
        "                    sc[c] = np.nan\n",
        "            return sc[out_cols]\n",
        "\n",
        "    msg(\"Querying SoilGrids for coarse sites (cached)...\")\n",
        "\n",
        "    rows = []\n",
        "    for _, r in site_tbl.iterrows():\n",
        "        sid = r[\"site_id\"]\n",
        "        lon = to_float(r[\"lon\"])\n",
        "        lat = to_float(r[\"lat\"])\n",
        "        fp = Path(cache_dir) / f\"soil_{sid}.json\"\n",
        "\n",
        "        if fp.exists():\n",
        "            try:\n",
        "                z = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
        "                row = {\"site_id\": sid, **z}\n",
        "                rows.append(row)\n",
        "                continue\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        zdf = soilgrids_point(lon, lat)\n",
        "        z = zdf.iloc[0].to_dict()\n",
        "        fp.write_text(json.dumps(z), encoding=\"utf-8\")\n",
        "        rows.append({\"site_id\": sid, **z})\n",
        "\n",
        "    sc = pd.DataFrame(rows)\n",
        "    for c in out_cols:\n",
        "        if c not in sc.columns:\n",
        "            sc[c] = np.nan\n",
        "    sc = sc[out_cols]\n",
        "    sc.to_csv(soil_cache_fp, index=False)\n",
        "    return sc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) PARALLEL: EDIT + RUN + CHECK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "def process_one_site(site_id: str, site_dir: str, apsimx_fp: str,\n",
        "                     do_run: bool = True,\n",
        "                     models_exe: Optional[str] = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Edit report, run APSIM (optional), then verify DB report exists.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ensure_report(apsimx_fp)\n",
        "    except Exception as e:\n",
        "        return {\"site_id\": site_id, \"site_dir\": site_dir, \"ok\": False, \"step\": \"edit_report\", \"msg\": str(e)}\n",
        "\n",
        "    if do_run:\n",
        "        try:\n",
        "            run_apsim_safely(apsimx_fp, models_exe=models_exe)\n",
        "        except Exception as e:\n",
        "            return {\"site_id\": site_id, \"site_dir\": site_dir, \"ok\": False, \"step\": \"run_apsim\", \"msg\": str(e)}\n",
        "\n",
        "    try:\n",
        "        df = read_report_from_db(site_dir)\n",
        "        if df is None or df.empty:\n",
        "            return {\"site_id\": site_id, \"site_dir\": site_dir, \"ok\": False, \"step\": \"read_db\",\n",
        "                    \"msg\": \"DB exists but no non-empty Report table found.\"}\n",
        "    except Exception as e:\n",
        "        return {\"site_id\": site_id, \"site_dir\": site_dir, \"ok\": False, \"step\": \"read_db\", \"msg\": str(e)}\n",
        "\n",
        "    return {\"site_id\": site_id, \"site_dir\": site_dir, \"ok\": True, \"step\": \"done\", \"msg\": \"\"}\n",
        "\n",
        "\n",
        "def run_all_sites(run_index: pd.DataFrame,\n",
        "                  do_run: bool = True,\n",
        "                  models_exe: Optional[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run all sites with a small parallel pool.\n",
        "    \"\"\"\n",
        "    msg(f\"Processing sites (do_run={do_run}) using ~{N_CORES} core(s)...\")\n",
        "    try:\n",
        "        from joblib import Parallel, delayed\n",
        "        logs = Parallel(n_jobs=N_CORES)(\n",
        "            delayed(process_one_site)(\n",
        "                r.site_id, r.site_dir, r.apsimx_fp, do_run=do_run, models_exe=models_exe\n",
        "            )\n",
        "            for r in run_index.itertuples(index=False)\n",
        "        )\n",
        "    except Exception:\n",
        "        # fallback serial\n",
        "        logs = [process_one_site(r.site_id, r.site_dir, r.apsimx_fp, do_run=do_run, models_exe=models_exe)\n",
        "                for r in run_index.itertuples(index=False)]\n",
        "    return pd.DataFrame(logs)\n",
        "\n",
        "\n",
        "# Set DO_RUN_APSIM=False if you already have .db outputs from elsewhere (recommended for Colab)\n",
        "DO_RUN_APSIM = False\n",
        "MODELS_EXE = None  # e.g. \"/usr/local/bin/Models\"\n",
        "\n",
        "run_log = run_all_sites(run_index, do_run=DO_RUN_APSIM, models_exe=MODELS_EXE)\n",
        "run_log.to_csv(os.path.join(OUT_DIR, \"run_log.csv\"), index=False)\n",
        "msg(\"Run success:\", int(run_log[\"ok\"].sum()), \"/\", len(run_log))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) COLLECT COARSE APSIM OUTPUTS (max Yield/LAI/Biomass)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Collecting coarse outputs (max Yield/LAI/Biomass)...\")\n",
        "\n",
        "rows = []\n",
        "for r in run_index.itertuples(index=False):\n",
        "    ok = bool(run_log.loc[run_log[\"site_id\"] == r.site_id, \"ok\"].iloc[0])\n",
        "    if not ok:\n",
        "        continue\n",
        "    df = read_report_from_db(r.site_dir)\n",
        "    if df is None or df.empty:\n",
        "        continue\n",
        "\n",
        "    def get_col(name):\n",
        "        if name not in df.columns:\n",
        "            return np.full(len(df), np.nan)\n",
        "        return pd.to_numeric(df[name], errors=\"coerce\").to_numpy(dtype=float)\n",
        "\n",
        "    Yield = get_col(\"Yield\")  # kg/ha because g/m2*10\n",
        "    LAI = get_col(\"LAI\")\n",
        "    Biomass = get_col(\"Biomass\")\n",
        "\n",
        "    Yield_t_ha = Yield / 1000.0\n",
        "\n",
        "    rows.append({\n",
        "        \"site_id\": r.site_id,\n",
        "        \"Yield_max_t_ha\": float(np.nanmax(Yield_t_ha)),\n",
        "        \"LAI_max\": float(np.nanmax(LAI)),\n",
        "        \"Biomass_max\": float(np.nanmax(Biomass)),\n",
        "    })\n",
        "\n",
        "coarse_out = pd.DataFrame(rows)\n",
        "coarse_out.to_csv(os.path.join(OUT_DIR, \"coarse_apsim_summary.csv\"), index=False)\n",
        "msg(\"Coarse summary rows:\", len(coarse_out))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) COORDS + CLIMATE FEATURES FROM MET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Extracting site lon/lat + climate features from MET...\")\n",
        "\n",
        "coords = []\n",
        "for r in run_index.itertuples(index=False):\n",
        "    lon, lat = parse_met_lonlat(r.site_dir)\n",
        "    coords.append({\"site_id\": r.site_id, \"lon\": lon, \"lat\": lat})\n",
        "coords = pd.DataFrame(coords)\n",
        "\n",
        "clim_rows = []\n",
        "for r in run_index.itertuples(index=False):\n",
        "    ok = bool(run_log.loc[run_log[\"site_id\"] == r.site_id, \"ok\"].iloc[0])\n",
        "    if not ok:\n",
        "        continue\n",
        "    cf = compute_climate_features_site(r.site_id, r.site_dir, season_months=SEASON_MONTHS)\n",
        "    if cf is not None:\n",
        "        clim_rows.append(cf)\n",
        "clim_feat = pd.concat(clim_rows, ignore_index=True) if clim_rows else pd.DataFrame({\"site_id\": []})\n",
        "\n",
        "coarse_site = (\n",
        "    coarse_out.merge(coords, on=\"site_id\", how=\"left\")\n",
        "             .merge(clim_feat, on=\"site_id\", how=\"left\")\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) SOIL FEATURES (SoilGrids)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "if DO_SOILGRIDS:\n",
        "    msg(\"Adding soil predictors (SoilGrids)...\")\n",
        "    soil_in = coarse_site[[\"site_id\", \"lon\", \"lat\"]].copy()\n",
        "    soil_in = soil_in[np.isfinite(soil_in[\"lon\"]) & np.isfinite(soil_in[\"lat\"])]\n",
        "    soil_feat = get_soil_features_for_sites(soil_in, cache_dir=CACHE_DIR)\n",
        "    coarse_site = coarse_site.merge(soil_feat, on=\"site_id\", how=\"left\")\n",
        "\n",
        "coarse_site.to_csv(os.path.join(OUT_DIR, \"coarse_site_features_all.csv\"), index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) BUILD EGYPT 0.1° GRID (centers inside Egypt polygon)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Building Egypt 0.1° grid...\")\n",
        "\n",
        "# Natural Earth countries via geopandas datasets\n",
        "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
        "egy = world[world[\"name\"] == \"Egypt\"].to_crs(\"EPSG:4326\")\n",
        "\n",
        "minx, miny, maxx, maxy = egy.total_bounds\n",
        "lon_vals = np.arange(math.floor(minx / RES_FINE) * RES_FINE, maxx + RES_FINE, RES_FINE)\n",
        "lat_vals = np.arange(math.floor(miny / RES_FINE) * RES_FINE, maxy + RES_FINE, RES_FINE)\n",
        "\n",
        "pts = []\n",
        "for i, lon in enumerate(lon_vals):\n",
        "    for j, lat in enumerate(lat_vals):\n",
        "        p = Point(float(lon), float(lat))\n",
        "        if egy.geometry.iloc[0].contains(p):\n",
        "            pts.append((lon, lat))\n",
        "\n",
        "fine = pd.DataFrame(pts, columns=[\"lon\", \"lat\"])\n",
        "fine[\"cell_id\"] = [f\"EGF_{i:06d}\" for i in range(1, len(fine) + 1)]\n",
        "msg(\"Fine grid points inside Egypt:\", len(fine))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) ML TRAINING (coords + climate + soil) + PREDICT to 0.1°\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Training ML model for yield downscaling (coords + climate + soil)...\")\n",
        "\n",
        "train_df = coarse_site.copy()\n",
        "train_df = train_df[np.isfinite(train_df[\"Yield_max_t_ha\"]) & np.isfinite(train_df[\"lon\"]) & np.isfinite(train_df[\"lat\"])]\n",
        "\n",
        "if len(train_df) < 10:\n",
        "    msg(\"WARNING: Too few training points. Will downscale by IDW Yield only.\", len(train_df))\n",
        "\n",
        "cand = [\n",
        "    \"lon\", \"lat\",\n",
        "    \"DD35\", \"HDW\", \"tmax_mean\", \"tmin_mean\", \"tmean_mean\", \"tmax_p95\", \"tmin_p05\",\n",
        "    \"radn_mean\", \"rain_sum\", \"rain_p95\", \"rh_mean\", \"wind_mean\", \"wind_p95\",\n",
        "] + [f\"{p}_0_30\" for p in SOIL_PROPS]\n",
        "\n",
        "cand = [c for c in cand if c in train_df.columns]\n",
        "\n",
        "def enough_values(col):\n",
        "    v = pd.to_numeric(train_df[col], errors=\"coerce\")\n",
        "    return np.isfinite(v).sum() >= max(10, int(0.5 * len(train_df)))\n",
        "\n",
        "ok_pred = [c for c in cand if enough_values(c)]\n",
        "ok_pred = list(dict.fromkeys([\"lon\", \"lat\"] + [c for c in ok_pred if c not in (\"lon\", \"lat\")]))\n",
        "\n",
        "msg(\"Predictors used:\", len(ok_pred), \",\".join(ok_pred))\n",
        "\n",
        "for v in ok_pred:\n",
        "    train_df[v] = impute_median(train_df[v])\n",
        "\n",
        "src_xy = xy_matrix(train_df[\"lon\"].to_numpy(), train_df[\"lat\"].to_numpy())\n",
        "trg_xy = xy_matrix(fine[\"lon\"].to_numpy(), fine[\"lat\"].to_numpy())\n",
        "\n",
        "# interpolate all non-coordinate predictors to fine grid\n",
        "for v in [x for x in ok_pred if x not in (\"lon\", \"lat\")]:\n",
        "    try:\n",
        "        fine[v] = idw_knn(src_xy, train_df[v].to_numpy(dtype=float), trg_xy)\n",
        "    except Exception:\n",
        "        fine[v] = np.nan\n",
        "    fine[v] = impute_median(fine[v])\n",
        "\n",
        "if len(train_df) >= 10:\n",
        "    X = train_df[ok_pred].to_numpy(dtype=float)\n",
        "    y = train_df[\"Yield_max_t_ha\"].to_numpy(dtype=float)\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=900,\n",
        "        random_state=123,\n",
        "        n_jobs=min(8, N_CORES),\n",
        "        bootstrap=True,\n",
        "    )\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    # Save model (simple joblib)\n",
        "    try:\n",
        "        import joblib\n",
        "        joblib.dump(rf, os.path.join(OUT_DIR, \"rf_model_yield.joblib\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    fine[\"Yield_pred_t_ha\"] = rf.predict(fine[ok_pred].to_numpy(dtype=float))\n",
        "else:\n",
        "    fine[\"Yield_pred_t_ha\"] = idw_knn(src_xy, train_df[\"Yield_max_t_ha\"].to_numpy(dtype=float), trg_xy)\n",
        "\n",
        "fine[\"Fail\"] = fine[\"Yield_pred_t_ha\"] < YIELD_FAIL_THRESHOLD\n",
        "fine.to_csv(os.path.join(OUT_DIR, \"yield_baseline_0p1deg.csv\"), index=False)\n",
        "msg(\"Saved baseline 0.1° yield.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) UNCERTAINTY + FAIL PROBABILITY (bootstrap)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "if DO_BOOTSTRAP_UNCERTAINTY and len(train_df) >= 10:\n",
        "    msg(\"Bootstrap uncertainty (n_boot=%d) ...\" % N_BOOT)\n",
        "    X = train_df[ok_pred].to_numpy(dtype=float)\n",
        "    y = train_df[\"Yield_max_t_ha\"].to_numpy(dtype=float)\n",
        "    Xp = fine[ok_pred].to_numpy(dtype=float)\n",
        "\n",
        "    preds = np.empty((len(fine), N_BOOT), dtype=float)\n",
        "    rng = np.random.default_rng(999)\n",
        "\n",
        "    for b in range(N_BOOT):\n",
        "        rf_b = RandomForestRegressor(\n",
        "            n_estimators=450,\n",
        "            random_state=1000 + b,\n",
        "            n_jobs=min(6, N_CORES),\n",
        "            bootstrap=True,\n",
        "        )\n",
        "        rf_b.fit(X, y)\n",
        "        preds[:, b] = rf_b.predict(Xp)\n",
        "\n",
        "    fine[\"Yield_mean_t_ha\"] = np.nanmean(preds, axis=1)\n",
        "    fine[\"Yield_sd_t_ha\"] = np.nanstd(preds, axis=1)\n",
        "    fine[\"P_fail\"] = np.nanmean(preds < YIELD_FAIL_THRESHOLD, axis=1)\n",
        "\n",
        "    fine.to_csv(os.path.join(OUT_DIR, \"yield_baseline_0p1deg_with_uncertainty.csv\"), index=False)\n",
        "    msg(\"Saved baseline + uncertainty + P_fail.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) CMIP6 IMPACTS (fallback)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Computing CMIP6 scenario impacts (fallback)...\")\n",
        "\n",
        "base_DD35 = fine[\"DD35\"] if \"DD35\" in fine.columns else pd.Series(np.zeros(len(fine)))\n",
        "base_HDW  = fine[\"HDW\"]  if \"HDW\"  in fine.columns else pd.Series(np.zeros(len(fine)))\n",
        "base_DD35 = impute_median(base_DD35)\n",
        "base_HDW  = impute_median(base_HDW)\n",
        "\n",
        "# cartesian join\n",
        "yield_cc = fine.assign(_k=1).merge(CMIP6.assign(_k=1), on=\"_k\").drop(columns=\"_k\")\n",
        "yield_cc[\"DD35_f\"] = base_DD35.to_numpy() + yield_cc[\"dT\"].to_numpy() * 30.0\n",
        "yield_cc[\"HDW_f\"]  = base_HDW.to_numpy()  + yield_cc[\"dT\"].to_numpy() * 10.0\n",
        "yield_cc[\"Yield_CC_t_ha\"] = yield_cc[\"Yield_pred_t_ha\"] - 0.015*(yield_cc[\"DD35_f\"]/30.0) - 0.05*(yield_cc[\"HDW_f\"]/10.0)\n",
        "yield_cc[\"Fail_CC\"] = yield_cc[\"Yield_CC_t_ha\"] < YIELD_FAIL_THRESHOLD\n",
        "\n",
        "yield_cc.to_csv(os.path.join(OUT_DIR, \"yield_cmip6_scenarios.csv\"), index=False)\n",
        "msg(\"Saved CMIP6 scenario yield impacts.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) EDA (maps, distributions, importance, correlation)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "\n",
        "msg(\"Saving EDA outputs...\")\n",
        "\n",
        "# 16.1 Baseline yield map (scatter)\n",
        "plt.figure()\n",
        "plt.scatter(fine[\"lon\"], fine[\"lat\"], s=1, c=fine[\"Yield_pred_t_ha\"])\n",
        "plt.title(\"Baseline maize yield (0.1°) - ML (coords + climate + soil)\")\n",
        "plt.xlabel(\"lon\"); plt.ylabel(\"lat\")\n",
        "plt.colorbar(label=\"t/ha\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"EDA_map_yield_baseline.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# 16.2 Histogram + density (simple)\n",
        "plt.figure()\n",
        "plt.hist(fine[\"Yield_pred_t_ha\"].to_numpy(dtype=float), bins=60)\n",
        "plt.title(\"Yield histogram (downscaled 0.1°)\")\n",
        "plt.xlabel(\"t/ha\"); plt.ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"EDA_hist_yield_baseline.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# 16.3 Training points\n",
        "if len(train_df) > 0:\n",
        "    plt.figure()\n",
        "    plt.scatter(train_df[\"lon\"], train_df[\"lat\"], s=20, c=train_df[\"Yield_max_t_ha\"])\n",
        "    plt.title(\"Coarse APSIM yield (training points)\")\n",
        "    plt.xlabel(\"lon\"); plt.ylabel(\"lat\")\n",
        "    plt.colorbar(label=\"t/ha\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"EDA_coarse_training_points.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# 16.4 RF importance (if model exists)\n",
        "try:\n",
        "    imp = pd.Series(rf.feature_importances_, index=ok_pred).sort_values(ascending=False)\n",
        "    imp.to_csv(os.path.join(OUT_DIR, \"EDA_rf_importance_table.csv\"), header=[\"importance\"])\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    imp.iloc[:20][::-1].plot(kind=\"barh\")\n",
        "    plt.title(\"RF feature importance (top 20)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"EDA_rf_importance.png\"), dpi=160)\n",
        "    plt.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 16.5 Correlation heatmap (coarse)\n",
        "eda_cols = [\"Yield_max_t_ha\"] + ok_pred\n",
        "eda_cols = [c for c in eda_cols if c in coarse_site.columns]\n",
        "eda_df = coarse_site[eda_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "keep = [c for c in eda_df.columns if np.isfinite(eda_df[c]).sum() >= 8]\n",
        "eda_df = eda_df[keep]\n",
        "if eda_df.shape[1] >= 3:\n",
        "    C = eda_df.corr()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(C, aspect=\"auto\")\n",
        "    plt.xticks(range(len(C.columns)), C.columns, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(C.index)), C.index)\n",
        "    plt.colorbar(label=\"corr\")\n",
        "    plt.title(\"Correlation heatmap (coarse: yield + predictors)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"EDA_correlation_heatmap_coarse.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# 16.6 P_fail map + Yield SD map (if uncertainty)\n",
        "if \"P_fail\" in fine.columns:\n",
        "    plt.figure()\n",
        "    plt.scatter(fine[\"lon\"], fine[\"lat\"], s=1, c=fine[\"P_fail\"])\n",
        "    plt.title(f\"Probability of yield failure (Yield < {YIELD_FAIL_THRESHOLD} t/ha)\")\n",
        "    plt.xlabel(\"lon\"); plt.ylabel(\"lat\")\n",
        "    plt.colorbar(label=\"P_fail\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"EDA_map_P_fail.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "if \"Yield_sd_t_ha\" in fine.columns:\n",
        "    plt.figure()\n",
        "    plt.scatter(fine[\"lon\"], fine[\"lat\"], s=1, c=fine[\"Yield_sd_t_ha\"])\n",
        "    plt.title(\"Prediction uncertainty (SD, t/ha)\")\n",
        "    plt.xlabel(\"lon\"); plt.ylabel(\"lat\")\n",
        "    plt.colorbar(label=\"SD\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, \"EDA_map_uncertainty_SD.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "msg(\"ALL DONE ✅\")\n",
        "msg(\"Outputs saved to:\", OUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}